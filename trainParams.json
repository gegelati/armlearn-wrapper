{
	// Use the progressive mode for choosing randomly the targets
	"progressiveModeTargets": true,
	// Initial maximum distance between the target and the starting position
	"maxLengthTargets": 30,
	// Allow to choose if the starting position are random of the BACKHOE one
	"doRandomStartingPosition": true,
	// Use the progressive mode for choosing randomly the starting position (no effect if doRandomStartingPosition is false)
	"progressiveModeStartingPos": true,
	// Initial maximum distance between the random starting position and the BACKHOE one
	"maxLengthStartingPos": 10,
	// Coefficient to upgrade the maxLengthStartingPos and MaxLengthTargets
	"coefficientUpgrade": 1.2,
	// Number of iterations to cumulate to upgrade the maxLengthStartingPos and MaxLengthTargets
	"nbIterationsUpgrade": 5,
	// Threshold reward to exceed to increment the counter of iteration to cumulate to upgrade the maxLengthStartingPos and MaxLengthTargets
	"thresholdUpgrade" : -8,
	// Allow to start with a predifined TPG
	"startPreviousTPG": false,
	// Name of the predifined TPG (no effect if startPreviousTPG is false)
	"namePreviousTPG": "out_1000.dot",
	// Proportion of trajectories (pair startingPos-Target) reused at each training generation
	"propTrajectoriesReused": 0.5,
	// Coefficient to choose the impact of the number of iterations taken by a TPG on the reward
	// The formula is : reward = init_reward - coefRewardNbIterations * (nbActionTaken / nbMaxActionTaken)
	"coefRewardNbIterations": 1
}